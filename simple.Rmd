---
title: "The complex inner life of simple regression"
author: "Matthew Rudd"
date: "Math for Data Science Conference, 12/1/20"
output:
  ioslides_presentation: default
  beamer_presentation: default
  slidy_presentation: default
---

## Pearson-Lee height data

```{r echo=F,warning=F,message=F}
library(tidyverse)
library(gghighlight)

heights <- read_csv("pearson.csv")
ensemble <- stack(heights)
ensemble.ave <- mean( ensemble$values )
ensemble.sd <- sd( ensemble$values )
heights <- heights %>% mutate(Group = round(Father))
son.stats <- heights %>% group_by(Group) %>% summarize(N = n(), Average = round(mean(Son),2), SD = round(sd(Son),2))
ggplot(ensemble,aes(values))+geom_histogram()+geom_vline(xintercept=ensemble.ave+ensemble.sd*c(-2,0,2),color="dodgerblue",linetype="dashed")+xlab("Heights of fathers and sons")

```

## Pearson-Lee height data

These heights are roughly $\, N(68.19, 2.82) \,$, so...

>- our best guess of a height is the average, `r round( ensemble.ave, 2 )` inches, 
>- which will likely be wrong, 
>- but probably not by more than 2 SDs, `r round( 2*ensemble.sd, 2 )` inches.
>- For better predictions, make more informed guesses!

## Pearson-Lee height data

```{r echo=F,warning=F,message=F}
ggplot( heights, aes(Father,Son) ) + geom_point()
```

## Pearson-Lee height data {.smaller}

```{r echo=F,warning=F,message=F}
son.table <- tibble("Father's rounded height"=son.stats$Group, "Number of sons"=son.stats$N, "Average height"=son.stats$Average, SD=son.stats$SD)
knitr::kable( son.table[6:14,] )
```

## Pearson-Lee height data

```{r echo=F,warning=F,message=F}
ggplot( heights, aes(Father,Son) ) + geom_point() + gghighlight(Group == 65)
```

## Pearson-Lee height data

```{r echo=F,warning=F,message=F}
ggplot( heights, aes(Father,Son) ) + geom_point() + gghighlight(Group == 71)
```

## Pearson-Lee height data

```{r echo=F,warning=F,message=F}
ggplot( heights, aes(Father,Son) ) + geom_point() + geom_point(data = son.stats, aes(Group, Average), color="deeppink", size=3)
```

## Pearson-Lee height data

```{r echo=F,warning=F,message=F}
ggplot( heights, aes(Father,Son) ) + geom_point() + geom_point(data = son.stats, aes(Group, Average), color="deeppink", size=3)+ geom_smooth( method="lm", level=0 )

```

## Pearson-Lee height data

The average height of a group of sons seems to depend _linearly_ on the father's given height.

>- Using this data, $$\text{Average height of sons} = 33.89 + .514 \times \text{Father's height} \, ,$$ $$\text{i.e.}, \quad E( y \mid x ) = 33.89 + .514 \, x \ .$$
>- This is _simple linear regression_, handled by `lm` in R :

## Pearson-Lee height data {.smaller}

```{r echo=F,warning=F,message=F}
fit <- lm( Son ~ Father, data = heights)
summary(fit)
```

## Pearson-Lee height data

This dataset illustrates regression -- and the regression effect -- nicely, but things are  actually _too nice_ in this example!

>- Both variables are roughly normally distributed, which can be misleading.
>- Many interesting datasets are too small to organize into groups and compute group means.

## UN data

Introduce the UN data

## The simple linear regression model

We use simple linear regression when we believe that $$y = \beta_{0} + \beta_{1} x + \epsilon \ . $$

>- **Assumptions**: the random error $\epsilon$ has _constant variance_ and _mean zero_.
>- **Consequence**: $$E(y \mid x) = \beta_{0} + \beta_{1}x$$
>- Also, assume that observations of the predictor are known _exactly_.

## The simple linear regression model

>- This model only makes sense if the mean response is a reasonable summary of $y$.
>- Neither the model parameters nor the additive error can be observed; they can only be estimated from observations.
>- Nothing is assumed to be normally distributed...
>- yet inferences and model assessment are still possible!

## The Gauss-Markov Theorem

>- Concisely: OLS is BLUE

## Markov's Inequality

## Chebyshev's Inequality

75% of observations are within 2 SDs -- no matter what!

## The coefficient of determination

blah

## Categorical data is not quantitative

blah

## The normal regression model

t-scores and whatnot, variable selection, ...

